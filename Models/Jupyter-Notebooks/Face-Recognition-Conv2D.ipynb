{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19e92aa-0ade-41d1-a8f1-c73cca1d8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:53:06.785824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-29 19:53:06.785851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-29 19:53:06.786684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-29 19:53:06.791247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import wandb \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57bcd56-b107-404e-b303-a1cb152c1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the GPU \n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:53:14.015624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:14.022718: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:14.023605: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU \\n\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "    \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d660e6e2-27d5-44ad-b1b1-30626548d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b306bfa8-e88f-45f1-adbb-9a0e60e0abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is a no-op as it is now the default behavior.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmdaz\u001b[0m (\u001b[33memmdaz-zzz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf14dc79-fd98-4f41-aab7-75d5202d01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize(img, (160, 160))\n",
    "    return img, label\n",
    "\n",
    "def make_dataset(paths, labels, batch_size = 32, shuffle = False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    ds = ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1e043d-3f70-4435-b102-d28b7cf6b7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image  identity\n",
       "0  000001.jpg         0\n",
       "1  000002.jpg         0\n",
       "2  000003.jpg         0\n",
       "3  000004.jpg         0\n",
       "4  000005.jpg         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se cargan los datos\n",
    "# Se debe de realizar una transformación en los datos, haciendo que todas las identidades sean 0:\n",
    "\n",
    "ds = pd.read_csv(\"/tf/Face-Recognition/CelebA/identity_CelebA.txt\", sep = r\"\\s+\", names=[\"image\", \"identity\"])\n",
    "\n",
    "ds[\"identity\"] = 0\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b1e1f7-10ec-4230-8a98-92d43f9cb17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>050310.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175279.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>011029.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>078879.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>012327.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image  identity\n",
       "0  050310.jpg         0\n",
       "1  175279.jpg         0\n",
       "2  011029.jpg         0\n",
       "3  078879.jpg         0\n",
       "4  012327.jpg         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img_dir = \"/tf/Face-Recognition/CelebA/img_align_celeba\"\n",
    "\n",
    "# ds[\"image_path\"] = ds[\"image\"].apply(lambda x: os.path.join(img_dir, x))\n",
    "ds = ds.sample(200, random_state = 4).reset_index(drop = True)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099fe841-d3c2-47b0-acb9-84d683e29975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>me1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me4.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>me132.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>me133.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>me134.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>me135.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>me136.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         image  identity\n",
       "0      me1.jpg         1\n",
       "1      me2.jpg         1\n",
       "2      me3.jpg         1\n",
       "3      me4.jpg         1\n",
       "4      me5.jpg         1\n",
       "..         ...       ...\n",
       "131  me132.jpg         1\n",
       "132  me133.jpg         1\n",
       "133  me134.jpg         1\n",
       "134  me135.jpg         1\n",
       "135  me136.jpg         1\n",
       "\n",
       "[136 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.array([])\n",
    "identity = np.zeros(136)\n",
    "\n",
    "for i in range(136):\n",
    "    image = np.append(image, f\"me{i+1}.jpg\")\n",
    "    identity[i] = 1\n",
    "\n",
    "identity = identity.astype(int)\n",
    "\n",
    "# print(image)\n",
    "# print(identity)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"image\": image,\n",
    "    \"identity\": identity,\n",
    "})\n",
    "\n",
    "# me_dir = \"/tf/Face-Recognition/CelebA/Me\"\n",
    "# df[\"image_path\"] = df[\"image\"].apply(lambda x: os.path.join(me_dir, x))\n",
    "\n",
    "# df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd3dc55-7674-4b3c-ba0d-689b3161a99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>015655.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161731.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me120.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me103.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152008.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>138118.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>010888.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>068816.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>054879.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>me7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image  identity\n",
       "0    015655.jpg         0\n",
       "1    161731.jpg         0\n",
       "2     me120.jpg         1\n",
       "3     me103.jpg         1\n",
       "4    152008.jpg         0\n",
       "..          ...       ...\n",
       "331  138118.jpg         0\n",
       "332  010888.jpg         0\n",
       "333  068816.jpg         0\n",
       "334  054879.jpg         0\n",
       "335     me7.jpg         1\n",
       "\n",
       "[336 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.concat([ds, df], axis = 0)\n",
    "ds = ds.sample(frac = 1, random_state = 5).reset_index(drop = True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909413ad-31e4-44bf-815d-aa1b7945f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_temp = train_test_split(\n",
    "    ds, test_size = 0.4, stratify = ds[\"identity\"], random_state = 5)\n",
    "\n",
    "df_val, df_test = train_test_split(\n",
    "    df_temp, test_size = 0.5, stratify = df_temp[\"identity\"], random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc16aa5a-30ea-4a9e-8fcd-b84302c5d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity\n",
      "0    120\n",
      "1     81\n",
      "Name: count, dtype: int64\n",
      "identity\n",
      "0    40\n",
      "1    27\n",
      "Name: count, dtype: int64\n",
      "identity\n",
      "0    40\n",
      "1    28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"identity\"].value_counts())\n",
    "print(df_val[\"identity\"].value_counts())\n",
    "print(df_test[\"identity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0520223c-0708-4425-8235-bd656900e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dataset_dir(base=\"data\"):\n",
    "    if os.path.exists(base):\n",
    "        shutil.rmtree(base)\n",
    "    os.makedirs(os.path.join(base, \"train/me\"))\n",
    "    os.makedirs(os.path.join(base, \"train/others\"))\n",
    "    os.makedirs(os.path.join(base, \"val/me\"))\n",
    "    os.makedirs(os.path.join(base, \"val/others\"))\n",
    "    os.makedirs(os.path.join(base, \"test/me\"))\n",
    "    os.makedirs(os.path.join(base, \"test/others\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0980521-d151-4853-8ace-9e485108936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "base = \"faces\"\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(f\"{base}/{split}/me\", exist_ok = True)\n",
    "    os.makedirs(f\"{base}/{split}/others\", exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "953cc9b0-c4b3-44df-b585-d14e8b62e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(df, split):\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"identity\"] == 1:\n",
    "            src = os.path.join(\"/tf/Face-Recognition/CelebA/Me\", row[\"image\"])\n",
    "            dst = f\"{base}/{split}/me/{row['image']}\"\n",
    "        else:\n",
    "            src = os.path.join(\"/tf/Face-Recognition/CelebA/img_align_celeba\", row[\"image\"])\n",
    "            dst = f\"{base}/{split}/others/{row['image']}\"\n",
    "        \n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e265fa8d-ac10-4cd3-bf23-e2f146a3ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dataset_dir()\n",
    "\n",
    "copy_files(df_train, \"train\")\n",
    "copy_files(df_val, \"val\")\n",
    "copy_files(df_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b00830cf-b215-464f-a3cb-dedf7936f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True)\n",
    "\n",
    "test_val_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34ac4e71-393a-4d2e-8865-f8997e6341b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 201 images belonging to 2 classes.\n",
      "Found 67 images belonging to 2 classes.\n",
      "Found 68 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_datagen.flow_from_directory(\n",
    "    f\"{base}/train\",\n",
    "    target_size = (160,160),\n",
    "    class_mode = \"binary\",\n",
    "    batch_size = 32,\n",
    "    seed = 4,\n",
    "    shuffle = True)\n",
    "\n",
    "val = test_val_datagen.flow_from_directory(\n",
    "    f\"{base}/val\",\n",
    "    target_size = (160,160),\n",
    "    class_mode = \"binary\",\n",
    "    batch_size = 32,\n",
    "    seed = 4,\n",
    "    shuffle = False)\n",
    "\n",
    "test = test_val_datagen.flow_from_directory(\n",
    "    f\"{base}/test\",\n",
    "    target_size = (160,160),\n",
    "    class_mode = \"binary\",\n",
    "    batch_size = 32,\n",
    "    seed = 4,\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e936b8-97a9-4ae3-a215-62c78ae486fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:53:15.085358: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:15.086273: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:15.086998: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:15.202893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:15.203791: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:15.204602: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-29 19:53:15.205346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2365 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "FeatureExtractor = tf.keras.models.load_model(\"/tf/Face-Recognition/Models/Conv2D-MobileNetV2-Based-Fine-Tunned.keras\")\n",
    "FeatureExtractor.trainable = False\n",
    "\n",
    "FeatureExtractor_Output = FeatureExtractor.layers[-3].output\n",
    "FeatureExtractor_Model = tf.keras.Model(FeatureExtractor.input, FeatureExtractor_Output)\n",
    "\n",
    "for layer in FeatureExtractor_Model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42ebd21c-c9b9-4938-a463-24d9c49913c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8960052d-3dcb-4829-bf7e-3ced08b451cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambd  (None, 160, 160, 3)       0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLamb  (None, 160, 160, 3)       0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " mobilenetv2_1.00_160 (Func  (None, 5, 5, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1280)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " classifier_head (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2775553 (10.59 MB)\n",
      "Trainable params: 148481 (580.00 KB)\n",
      "Non-trainable params: 2627072 (10.02 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(160,160,3))\n",
    "# x = FeatureExtractor_Model(inputs)\n",
    "x = tf.keras.layers.Dense(256, activation=\"leaky_relu\")(FeatureExtractor_Model.output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(256, activation = \"leaky_relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.15)(x)\n",
    "x = tf.keras.layers.Dense(256, activation = \"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation = \"sigmoid\", dtype = \"float32\",name = \"classifier_head\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs = FeatureExtractor_Model.input, outputs = outputs)\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56fe1459-9ad0-4dc1-8020-1531a74a6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = \"val_accuracy\", patience = 10, restore_best_weights = True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33f9a782-c535-445e-9387-637ec01750ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tf/Face-Recognition/Trials/wandb/run-20251129_195338-aggvxmxc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0/runs/aggvxmxc' target=\"_blank\">Trial_1_FullSet</a></strong> to <a href='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0/runs/aggvxmxc' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0/runs/aggvxmxc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0/runs/aggvxmxc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x726df85d9150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project = \"Face-Recognition-Conv2D-Trials-Exp-Series1.0\",\n",
    "        name = \"Trial_1_FullSet\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"activation\": \"leaky_relu, relu\",\n",
    "            \"n_layers\": 3,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"RMSProp\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "908e90a9-0877-4d03-866b-32457e790e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:53:42.349196: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "2025-11-29 19:53:44.508668: I external/local_xla/xla/service/service.cc:168] XLA service 0x726da8ab5080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-29 19:53:44.508690: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2025-11-29 19:53:44.513160: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764446024.545964   99803 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 8s 573ms/step - loss: 0.6853 - accuracy: 0.5423 - val_loss: 0.6439 - val_accuracy: 0.5970 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 2s 258ms/step - loss: 0.6559 - accuracy: 0.5970 - val_loss: 0.6215 - val_accuracy: 0.6119 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 2s 218ms/step - loss: 0.6310 - accuracy: 0.6169 - val_loss: 0.6018 - val_accuracy: 0.6119 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.6108 - accuracy: 0.6418 - val_loss: 0.5831 - val_accuracy: 0.7015 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 2s 219ms/step - loss: 0.5884 - accuracy: 0.6567 - val_loss: 0.5633 - val_accuracy: 0.7164 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 2s 266ms/step - loss: 0.5852 - accuracy: 0.6617 - val_loss: 0.5466 - val_accuracy: 0.7612 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 2s 232ms/step - loss: 0.5585 - accuracy: 0.7214 - val_loss: 0.5291 - val_accuracy: 0.7761 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.5296 - accuracy: 0.7214 - val_loss: 0.5100 - val_accuracy: 0.7612 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.5169 - accuracy: 0.7512 - val_loss: 0.4935 - val_accuracy: 0.8060 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.5143 - accuracy: 0.7562 - val_loss: 0.4815 - val_accuracy: 0.8657 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 2s 222ms/step - loss: 0.5076 - accuracy: 0.8109 - val_loss: 0.4668 - val_accuracy: 0.8806 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 2s 221ms/step - loss: 0.4779 - accuracy: 0.8109 - val_loss: 0.4511 - val_accuracy: 0.8806 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 2s 243ms/step - loss: 0.4678 - accuracy: 0.7910 - val_loss: 0.4399 - val_accuracy: 0.8657 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 2s 224ms/step - loss: 0.4566 - accuracy: 0.7811 - val_loss: 0.4234 - val_accuracy: 0.8657 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 2s 226ms/step - loss: 0.4482 - accuracy: 0.7711 - val_loss: 0.4189 - val_accuracy: 0.8507 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 2s 227ms/step - loss: 0.4411 - accuracy: 0.8209 - val_loss: 0.4030 - val_accuracy: 0.8657 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 2s 227ms/step - loss: 0.4041 - accuracy: 0.8607 - val_loss: 0.3943 - val_accuracy: 0.8507 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.4255 - accuracy: 0.8209 - val_loss: 0.3913 - val_accuracy: 0.8507 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 2s 228ms/step - loss: 0.4250 - accuracy: 0.8010 - val_loss: 0.3811 - val_accuracy: 0.8507 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 2s 222ms/step - loss: 0.3975 - accuracy: 0.8458 - val_loss: 0.3885 - val_accuracy: 0.8358 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 2s 242ms/step - loss: 0.3726 - accuracy: 0.8706 - val_loss: 0.3788 - val_accuracy: 0.8358 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train, \n",
    "    validation_data = val,\n",
    "    epochs = 200,\n",
    "    verbose = 1, \n",
    "    callbacks = [WandbMetricsLogger(log_freq = 5), early_stopping, lr_reduction]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a97489d8-b393-40a5-889b-1cf6b73c3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Face-Recognition-Conv2D.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a07fe355-9149-4ee9-aa38-bd1570f6cdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▃▅▅▅▅▅▅▆▅▅▅▅▆█▆▆▆▇▆▇█▇▇▇▆▇▇▆▆▇▇█▇▇▆▇▇▇▇</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>█▇▆▆▆▆▅▅▅▅▅▅▅▅▁▄▄▄▃▄▃▂▃▄▂▂▃▂▂▃▂▃▂▂▂▃▂▂▁▁</td></tr><tr><td>epoch/accuracy</td><td>▁▂▃▃▃▄▅▅▅▆▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▇▆▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▁▁▄▄▅▅▅▆█████▇█▇▇▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▇▇▆▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.85799</td></tr><tr><td>batch/batch_step</td><td>205</td></tr><tr><td>batch/learning_rate</td><td>0.0001</td></tr><tr><td>batch/loss</td><td>0.38521</td></tr><tr><td>epoch/accuracy</td><td>0.87065</td></tr><tr><td>epoch/epoch</td><td>20</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.37255</td></tr><tr><td>epoch/val_accuracy</td><td>0.83582</td></tr><tr><td>epoch/val_loss</td><td>0.37876</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Trial_1_FullSet</strong> at: <a href='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0/runs/aggvxmxc' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0/runs/aggvxmxc</a><br> View project at: <a href='https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Face-Recognition-Conv2D-Trials-Exp-Series1.0</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_195338-aggvxmxc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf8865-0dcb-4afc-b318-f3a342dc986c",
   "metadata": {},
   "source": [
    "### Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86af40b4-d9d2-4242-a544-a1b8318481f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in FeatureExtractor_Model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in FeatureExtractor_Model.layers[-5:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "568297ea-2af7-4479-a1c9-ac84b01ad3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-6)\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5bf16a7-7351-4194-94dd-a7452dabd218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tf/Face-Recognition/Trials/wandb/run-20251129_195422-wad5rgnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0/runs/wad5rgnm' target=\"_blank\">FineTunning_1_FullSet</a></strong> to <a href='https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0/runs/wad5rgnm' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0/runs/wad5rgnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 305ms/step - loss: 0.4933 - accuracy: 0.7811 - val_loss: 0.4664 - val_accuracy: 0.8806\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 2s 228ms/step - loss: 0.4740 - accuracy: 0.7960 - val_loss: 0.4661 - val_accuracy: 0.8806\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 2s 223ms/step - loss: 0.4791 - accuracy: 0.8209 - val_loss: 0.4659 - val_accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 2s 227ms/step - loss: 0.4916 - accuracy: 0.7612 - val_loss: 0.4657 - val_accuracy: 0.8806\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 2s 221ms/step - loss: 0.5000 - accuracy: 0.7910 - val_loss: 0.4655 - val_accuracy: 0.8806\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 2s 226ms/step - loss: 0.4857 - accuracy: 0.7960 - val_loss: 0.4652 - val_accuracy: 0.8806\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 2s 225ms/step - loss: 0.4880 - accuracy: 0.8109 - val_loss: 0.4650 - val_accuracy: 0.8806\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.4755 - accuracy: 0.7960 - val_loss: 0.4648 - val_accuracy: 0.8806\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.4964 - accuracy: 0.8010 - val_loss: 0.4645 - val_accuracy: 0.8806\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.4832 - accuracy: 0.7960 - val_loss: 0.4643 - val_accuracy: 0.8806\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project = \"Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0\",\n",
    "        name = \"FineTunning_1_FullSet\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"activation\": \"leaky_relu, relu\",\n",
    "            \"n_layers\": 3,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"RMSProp\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "history = model.fit(\n",
    "    train, \n",
    "    validation_data = val,\n",
    "    epochs = 10,\n",
    "    verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffef651c-36d6-4ccf-9259-18b6f1ad32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Face-Recognition-Conv2D-Fine-Tunned.keras\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fd07bb7-aee9-478a-a765-601d5fe145e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FineTunning_1_FullSet</strong> at: <a href='https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0/runs/wad5rgnm' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0/runs/wad5rgnm</a><br> View project at: <a href='https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0' target=\"_blank\">https://wandb.ai/emmdaz-zzz/Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_195422-wad5rgnm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "159ea05b-1be5-4278-a3b1-bcb5c1588cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 481ms/step - loss: 0.4688 - accuracy: 0.8529\n",
      "Test Loss: 0.4687805473804474\n",
      "Test Accuracy: 0.8529411554336548\n"
     ]
    }
   ],
   "source": [
    "model_ev = keras.models.load_model(\"Face-Recognition-Conv2D-Fine-Tunned.keras\")\n",
    "\n",
    "loss, accuracy = model_ev.evaluate(test, verbose = 1)\n",
    "\n",
    "print(f\"Test Loss: {loss:}\")\n",
    "print(f\"Test Accuracy: {accuracy:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ef18045-8725-41fa-8459-84cbb8848b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probs = model.predict(test)\n",
    "\n",
    "y_pred = (pred_probs > 0.5).astype(\"int32\")\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "y_true = test.classes\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot = True, fmt = \"d\", cmap = \"rocket\")\n",
    "\n",
    "plt.xlabel(\"Valor real\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce71dd-a32f-48e8-a7a3-312de6a8cdff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
