{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157ff8d5-6f45-43e4-a54f-5e0077947477",
   "metadata": {},
   "source": [
    "# Facial Atributes Recognition Model based on MobileNetV2\n",
    "This code creates a model to do facial atributes recognition which then will be used as a pretained model for a facial recognition model. \n",
    "This Conv2D model uses MobileNetV2 due to its lightness and rapidy during training. It was revised and discovered in **Vafaa Sukkar** and **Ergun Ercelebi** article **\"A Real-time Face Recognition Based on MobileNetV2 Model\"**. It can be found in https://ineseg.org/wp-content/uploads/2023/08/4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4b6f6-aafd-4dd8-bc15-a87558ad4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, regularizers, mixed_precision\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321f392-0d11-41cf-a6ce-b7e60912f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU \\n\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "    \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2a441-c428-47ac-8c73-a92428fc52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a15fd-09ed-4f15-b7f6-61fd271bb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13350d4a-6d95-4281-a8ea-fd20e837c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize(img, (160, 160))\n",
    "    return img, label\n",
    "\n",
    "def make_dataset(paths, labels, batch_size = 32, shuffle = False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    ds = ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442461c-ff87-4d01-bd4d-e92af45458ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los datos\n",
    "# Se debe de realizar una transformación en los datos, convirtiéndolos de -1/1 a 0/1:\n",
    "\n",
    "ds = pd.read_csv(\"/tf/Face-Recognition/CelebA/list_attr_celeba.txt\", sep = r\"\\s+\", skiprows = 1)\n",
    "ds.iloc[:, 0:40] = (ds.iloc[:, 0:40] == 1).astype(\"int32\")\n",
    "ds.head()\n",
    "# ds.describe()\n",
    "# (ds == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c91ffc-bfdf-4b6d-a442-a306766cde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/img_align_celeba\"\n",
    "\n",
    "ds[\"image_path\"] = ds.index.map(lambda x: os.path.join(img_dir, x))\n",
    "ds.reset_index(inplace = True)\n",
    "ds.rename(columns = {\"index\" : \"image\"}, inplace = True)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a9bc3-15a2-40ff-9c0a-55249d0e85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.read_csv(\n",
    "    \"/tf/Face-Recognition/CelebA/Eval/list_eval_partition.txt\", \n",
    "    sep = r\"\\s+\",\n",
    "    names = [\"image\", \"partition\"])\n",
    "\n",
    "ds = ds.merge(df_split, on = \"image\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4e4fd-e5d2-49a7-90b8-e72d42cdb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(ds[\"image_path\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e690ec-8069-497d-b467-c91a25817d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds[ds[\"partition\"] == 0]\n",
    "df_val   = ds[ds[\"partition\"] == 1]\n",
    "df_test  = ds[ds[\"partition\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdee92-1487-49e8-bae8-c0f15b4c4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(\n",
    "    df_train[\"image_path\"].values, \n",
    "    df_train.iloc[:, 1:41].values,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_ds = make_dataset(\n",
    "    df_val[\"image_path\"].values,\n",
    "    df_val.iloc[:, 1:41].values,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "test_ds = make_dataset(\n",
    "    df_test[\"image_path\"].values,\n",
    "    df_test.iloc[:, 1:41].values,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50360c8-5e92-4b9b-bfc0-729ace96ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small = df_train.sample(frac = 0.15, random_state = 5)\n",
    "df_val_small = df_val.sample(frac = 0.15, random_state = 5)\n",
    "df_test_small = df_test.sample(frac = 0.15, random_state = 5)\n",
    "\n",
    "train_ds_small = make_dataset(\n",
    "    df_train_small[\"image_path\"].values,\n",
    "    df_train_small.iloc[:, 1:41].values,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "val_ds_small = make_dataset(\n",
    "    df_val_small[\"image_path\"].values,\n",
    "    df_val_small.iloc[:, 1:41].values,\n",
    ")\n",
    "\n",
    "test_ds_small = make_dataset(\n",
    "    df_test_small[\"image_path\"].values,\n",
    "    df_test_small.iloc[:, 1:41].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99852c42-5a09-4cd5-9abc-d45d6753e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNetV2 = keras.applications.MobileNetV2(\n",
    "    weights = \"imagenet\",\n",
    "    input_shape = (160, 160, 3),\n",
    "    include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c8429-bfb3-4341-898c-76d86959cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNetV2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33359192-138e-4d14-8165-9a3e1aae682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32026a-17d6-45dd-bdf2-2e737cc2e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (160, 160, 3))\n",
    "x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "x = MobileNetV2(x, training = False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.15)(x)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(40, activation = \"sigmoid\", dtype = \"float32\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = optimizer,\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790dc92-ee8d-420b-ae24-6263ccf56792",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = \"val_binary_accuracy\", patience = 10, restore_best_weights = True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da826f1d-009c-454d-a4c9-c70193ce382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "        project = \"Conv2D-MobileNetV2-Based-Trials-Exp-Series1.0\",\n",
    "        name = \"Trial_1_FullSet\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"activation\": \"leaky_relu, relu\",\n",
    "            \"n_layers\": 3,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"RMSProp\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cce5b7-02bc-4c1d-bdf4-65f3561cf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data = val_ds,\n",
    "    epochs = 200,\n",
    "    verbose = 1, \n",
    "    callbacks = [WandbMetricsLogger(log_freq = 5), early_stopping, lr_reduction]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d490a5d-5d22-4b9c-9c16-efdeb295ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fd744-f5ee-49e3-acb3-ce026cdb32c7",
   "metadata": {},
   "source": [
    "### Multi-label (CelebA) considerations\n",
    "\n",
    "CelebA is **multi-label** each image has 40 independent binary attributes. Important implications:\n",
    "\n",
    "- **BinaryAccuracy:** When using a single BinaryAccuracy() metric in model.compile, it computes accuracy element-wise over the whole output tensor. Concretely:\n",
    "\n",
    "    - If batch size = 32 and 40 labels, each update_state compares 32×40 = 1280 elements and returns fraction correct out of 1280.\n",
    "\n",
    "    - This is typically more informative than “exact match accuracy” (which requires all 40 to be correct per image and is often near zero).\n",
    "\n",
    "- **AUC for multi-label:** AUC can be evaluated:\n",
    "\n",
    "    - **Per-class AUC:** Computes AUC separately for each of the 40 labels (which is recommended). This reveals which attributes the model predicts well and which it doesn’t.\n",
    "\n",
    "    - **Micro AUC:** Treats every label-instance pair as separate samples (flatten all labels and predictions) and computes a single AUC across them (equivalent to element-wise pooling). This weights attributes by frequency.\n",
    "\n",
    "    - **Macro AUC:** Averages per-class AUC equally (gives each attribute equal weight regardless of prevalence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3018eba-968d-4af7-b3e7-7bad2c8e4314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
